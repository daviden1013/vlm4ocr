{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#vlm4ocr","title":"VLM4OCR","text":"<p>vlm4ocr is a toolkit for Optical character recognition (OCR) with Vision language models (VLMs). In includes three components:</p> <ul> <li>Web Application for drag-and-drop access</li> <li>CLI for command line access</li> <li>Python package for Python access</li> </ul>"},{"location":"cli/","title":"CLI","text":"<p>Command line interface (CLI) provides an easy way to batch process many images, PDFs, and TIFFs in a directory. </p>"},{"location":"cli/#installation","title":"Installation","text":"<p>Install the Python package on PyPi and the CLI tool will be automatically installed. </p><pre><code>pip install vlm4ocr\n</code></pre>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>Run OCR for all supported file types in the <code>/examples/synthesized_data/</code> folder with a locally deployed Qwen2.5-VL-7B-Instruct and generate results as markdown. OCR results and a log file (enabled by <code>--log</code>) will be written to the <code>output_path</code>. <code>--concurrent_batch_size</code> deternmines the number of images/pages can be processed at a time. This is good for managing resources.  </p><pre><code># OpenAI compatible API\nvlm4ocr --input_path /examples/synthesized_data/ \\\n        --output_path /examples/ocr_output/ \\\n        --skip_existing \\\n        --output_mode markdown \\\n        --log \\\n        --max_dimension_pixels 4000 \\\n        --vlm_engine openai_compatible \\\n        --model Qwen/Qwen2.5-VL-7B-Instruct \\\n        --api_key EMPTY \\\n        --base_url http://localhost:8000/v1 \\\n        --concurrent_batch_size 4\n</code></pre> <p>Use gpt-4o-mini to process a PDF with many pages. Since <code>--output_path</code> is not specified, outputs and logs will be written to the current work directory.  </p><pre><code># OpenAI API\nexport OPENAI_API_KEY=&lt;api key&gt;\nvlm4ocr --input_path /examples/synthesized_data/GPT-4o_synthesized_note_1.pdf \\\n        --output_mode HTML \\\n        --log \\\n        --vlm_engine openai \\\n        --model gpt-4o-mini \\\n        --concurrent_batch_size 4\n</code></pre>"},{"location":"cli/#usage","title":"Usage","text":"<p>The CLI parameters are grouped into categories to manage the OCR process.</p>"},{"location":"cli/#inputoutput-options","title":"Input/Output Options","text":"<ul> <li><code>--input_path</code> Specify a single input file or a directory with multiple files for OCR.</li> <li><code>--output_mode</code> Should be one of <code>text</code>, <code>markdown</code>, or <code>HTML</code>.</li> <li><code>--output_path</code> If input_path is a directory of multiple files, this should be an output directory. If input is a single file, this can be a full file path or a directory. If not provided, results are saved to the current working directory. </li> <li><code>--skip_existing</code> Skip processing files that already have OCR results in the output directory. If False, all input files will be processed and potentially overwrite existing outputs.</li> </ul>"},{"location":"cli/#image-processing-parameters","title":"Image Processing Parameters","text":"<ul> <li><code>--rotate_correction</code> Apply automatic rotation correction for input images. This requires Tesseract OCR to be installed and configured correctly. (default: False)</li> <li><code>--max_dimension_pixels</code> Maximum dimension (width or height) in pixels for input images. Images larger than this will be resized to fit within this limit while maintaining aspect ratio. (default: 4000)</li> </ul>"},{"location":"cli/#vlm-engine-selection","title":"VLM Engine Selection","text":"<ul> <li><code>--vlm_engine</code> Should be one of <code>openai</code>, <code>azure_openai</code>, <code>ollama</code>, or <code>openai_compatible</code>.</li> <li><code>--model</code> VLM model name.</li> <li><code>--max_new_tokens</code> Set maximum output tokens (default: 4096).</li> <li><code>--temperature</code> Set temperature (default: 0.0).</li> </ul>"},{"location":"cli/#openai-openai-compatible-options","title":"OpenAI &amp; OpenAI-Compatible Options","text":"<ul> <li><code>--api_key</code> API key. Can be set though environmental variable.</li> <li><code>--base_url</code> Base URL.</li> </ul>"},{"location":"cli/#azure-openai-options","title":"Azure OpenAI Options","text":"<ul> <li><code>--azure_api_key</code> Azure API key. Can be set though environmental variable. </li> <li><code>--azure_endpoint</code> Azure endpoint. Can be set though environmental variable. </li> <li><code>--azure_api_version</code> Azure API version. </li> </ul>"},{"location":"cli/#ollama-options","title":"Ollama Options","text":"<ul> <li><code>--ollama_host</code> Ollama host:port (default: http://localhost:11434).</li> <li><code>--ollama_num_ctx</code> Ollama context length (default: 4096).</li> <li><code>--ollama_keep_alive</code> Ollama keep_alive seconds. (default: 300)</li> </ul>"},{"location":"cli/#ocr-engine-parameters","title":"OCR Engine Parameters","text":"<ul> <li><code>--user_prompt</code> Specify custom user prompt.</li> </ul>"},{"location":"cli/#processing-options","title":"Processing Options","text":"<ul> <li><code>--concurrent_batch_size</code> Number of images/pages to process concurrently. Set to 1 for sequential processing of VLM calls. (default: 4)</li> <li><code>max_file_load</code> Number of input files to pre-load. Set to -1 for automatic config: 2 * concurrent_batch_size. </li> <li><code>--log</code> Enable writing logs to a timestamped file in the output directory. (default: False)</li> <li><code>--debug</code> Enable debug level logging for console (and file if --log is active). (default: False)</li> </ul>"},{"location":"ocr_engines/","title":"OCR Engine","text":"<p>OCR engine handles the OCR process. It supports batch processing which is ideal for large amount of input files; sequential processing which is good for lightweight tasks or testing; streaming that streams OCR results which is designed for frontend integration. </p> <p>OCR engine requires an VLM engine instance and an <code>output_mode</code> as one of markdown, HTML, or text. The optional <code>user_prompt</code> can be used to provide additional information about the input files. For example, The input is a scanned MRI report. </p> <pre><code>from vlm4ocr import OCREngine\n\nocr = OCREngine(vlm_engine=vlm_engine, \n                output_mode=\"markdown\", \n                user_prompt=\"&lt;additional information about the input files&gt;\")\n</code></pre> <p><code>system_prompt</code> can be customized. But we recommend using the default (<code>system_prompt=None</code> or omit) since it controls the output mode and post-processing. Below is the system prompt for markdown output mode:</p> <pre><code>You are a helpful assistant that can convert scanned documents \\\ninto markdown text. Your output is accurate and well-formatted, \\\nstarting with ```markdown and ending with ```. You will only \\\noutput the markdown text without any additional explanations or comments. \\\nThe markdown should include all text, tables, and lists with appropriate \\\nheaders (e.g., \"##\"). You will ignore images, icons, or anything that can \\\nnot be converted into text.\n</code></pre>"},{"location":"ocr_engines/#batch-ocr","title":"Batch OCR","text":"<p><code>concurrent_ocr</code> is the recommended method to process large amount of files. The method returns an async generator of <code>OCRResult</code> instance (<code>AsyncGenerator[OCRResult, None]</code>). OCR results are generated whenever is ready (first-done-first-out). There is no guarantee the input order and output order will match. Use the <code>OCRResult.filename</code> as identifier. The <code>file_paths</code> is a single file (image, PDF, TIFF) or a list of file directories. <code>rotate_correction</code> use Tesseract to correct for rotation. Please install Tesseract to use this feature. <code>max_dimension_pixels</code> resize images to ensure the largest dimension (width or length) are less than the maximum allowed pixels. <code>concurrent_batch_size</code> is the number of images/pages that VLM processes at a time. This is used to manage inferencing resource (usually GPU). The <code>max_file_load</code> is the number of input files to be pre-loaded for staging. This manages the I/O and memory (dRAM) resources. By default, <code>max_file_load</code> is twice of <code>concurrent_batch_size</code>. </p> <p>The code below runs OCR in batches of 4 images/pages, while having 8 files pre-loaded to ensure efficiency.  </p><pre><code>response = ocr.concurrent_ocr(file_paths=&lt;a list of files&gt;, \n                              rotate_correction=True,\n                              max_dimension_pixels=4000,\n                              concurrent_batch_size=4,\n                              max_file_load=8)\n</code></pre>"},{"location":"ocr_engines/#example-dynamic-output-writing","title":"Example: dynamic output-writing","text":"<p>The example below use <code>concurrent_ocr</code> to perform OCR and write available results to file.</p> <pre><code>import asyncio\n\nasync def run_ocr():\n    response = ocr.concurrent_ocr(&lt;list of files&gt;, concurrent_batch_size=4)\n    async for result in response:\n        if result.status == \"success\":\n            filename = result.filename\n            ocr_text = result.to_string()\n            with open(f\"{filename}.md\", \"w\", encoding=\"utf-8\") as f:\n                f.write(ocr_text)\n\nasyncio.run(run_ocr())\n</code></pre>"},{"location":"ocr_engines/#sequential-ocr","title":"Sequential OCR","text":"<p><code>sequential_ocr</code> is a lightweight method to perform OCR. Input files are processed page by page, file by file sequentially. This is suitable for small-scaled tasks or testing. The <code>verbose=True</code> streams the OCR results in console. </p> <pre><code># OCR for a single image\nocr_results = ocr.sequential_ocr(image_path, verbose=True)\n\n# OCR for a single pdf (multiple pages)\nocr_results = ocr.sequential_ocr(pdf_path, verbose=True)\n\n# OCR for multiple image and pdf files\nocr_results = ocr.sequential_ocr([pdf_path, image_path], verbose=True)\n\n# Inspect OCR results\nlen(ocr_results) # 2 files\nocr_results[0].input_dir\nocr_results[0].filename\nlen(ocr_results[0]) # PDF file number of pages\nocr_text = ocr_results[0].to_string() # OCR text (all pages concatenated)\n</code></pre>"},{"location":"ocr_engines/#stream-ocr","title":"Stream OCR","text":"<p><code>stream_ocr</code> method is designed for frontend integration. It outputs a generator of chunk dictionary (<code>Generator[Dict[str, str], None, None]</code>). For OCR output tokens, it yields: {\"type\": \"ocr_chunk\", \"data\": chunk}. For page delimitors, it yields: {\"type\": \"page_delimiter\", \"data\": page_delimiter}. </p> <pre><code>response = ocr.stream_ocr(image_path)\n\nfor chunk in response:\n    if chunk[\"type\"] == \"ocr_chunk\":\n        print(chunk[\"data\"], end=\"\", flush=True)\n    elif chunk[\"type\"] == \"page_delimiter\":\n        print(chunk[\"data\"])\n</code></pre>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#installation","title":"Installation","text":"<p>Python package is available on PyPi. </p><pre><code>pip install vlm4ocr\n</code></pre>"},{"location":"quick_start/#quick-start","title":"Quick start","text":"<p>In this demo, we use a locally deployed vLLM OpenAI compatible server to run Qwen2.5-VL-7B-Instruct. For more inference APIs and VLMs, please see VLMEngine. </p> <pre><code>from vlm4ocr import OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"Qwen/Qwen2.5-VL-7B-Instruct\", \n                             base_url=\"http://localhost:8000/v1\", \n                             api_key=\"EMPTY\")\n</code></pre> <p>We define OCR engine and specify output formats.</p> <pre><code>from vlm4ocr import OCREngine\n\n# Image/PDF paths\nimage_path = \"/examples/synthesized_data/GPT-4o_synthesized_note_1_page_1.jpg\"\npdf_path = \"/examples/synthesized_data/GPT-4o_synthesized_note_1.pdf\"\n\n# Define OCR engine\nocr = OCREngine(vlm_engine, output_mode=\"markdown\")\n</code></pre>"},{"location":"quick_start/#run-ocr-sequentially","title":"Run OCR sequentially","text":"<p>We run OCR sequentially (process one image at a time) for single or multiple files. This approach is suitable for testing or processing small-scaled requests.</p> <pre><code># OCR for a single image\nocr_results = ocr.sequential_ocr(image_path, verbose=True)\n\n# OCR for a single pdf (multiple pages)\nocr_results = ocr.sequential_ocr(pdf_path, verbose=True)\n\n# OCR for multiple image and pdf files\nocr_results = ocr.sequential_ocr([pdf_path, image_path], verbose=True)\n\n# Inspect OCR results\nlen(ocr_results) # 2 files\nocr_results[0].input_dir # input dir\nocr_results[0].filename # input filename\nocr_results[0].status # OCR result status: 'success'\nlen(ocr_results[0]) # PDF file number of pages\nocr_text = ocr_results[0].to_string() # OCR text (all pages concatenated)\n</code></pre>"},{"location":"quick_start/#run-ocr-concurrently","title":"Run OCR concurrently","text":"<p>For high-volume OCR tasks, it is more efficient to run OCR concurrently. The example below concurrently processes 4 images/pages at a time and write outputs to file whenever a file has finished. </p> <pre><code>import asyncio\n\nasync def run_ocr():\n    response = ocr.concurrent_ocr([image_path_1, image_path_2], concurrent_batch_size=4)\n    async for result in response:\n        if result.status == \"success\":\n            filename = result.filename\n            ocr_text = result.to_string()\n            with open(f\"{filename}.md\", \"w\", encoding=\"utf-8\") as f:\n                f.write(ocr_text)\n\nasyncio.run(run_ocr())\n</code></pre>"},{"location":"vlm_engines/","title":"VLM Engines","text":"<p>The <code>VLMEngine</code> class is responsible for configuring VLM for OCR. Children of this abstract class implements <code>chat</code> and <code>chat_async</code> methods for prompting VLMs with input messages. It also has <code>get_ocr_messages</code> method that unifies messages template for image input. Below are the built-in VLMEngines. Use <code>BasicVLMConfig</code> to set sampling parameters. For OpenAI reasoning models (\"o\" series), use <code>OpenAIReasoningVLMConfig</code> to automatically handle system prompt. </p>"},{"location":"vlm_engines/#openai-compatible","title":"OpenAI Compatible","text":"<p>The OpenAI compatible VLM engine works with a wide variety of VLM inferencing services:</p> <pre><code>from vlm4ocr import OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"&lt;mode_name&gt;\", base_url=\"&lt;base_url&gt;\", api_key=\"&lt;api_key&gt;\")\n</code></pre>"},{"location":"vlm_engines/#locally-hosted-vllm-server","title":"Locally hosted vLLM server","text":"<p>Inference engines like vLLM OpenAI compatible server is supported. To start a server:</p> <pre><code>vllm serve Qwen/Qwen2.5-VL-7B-Instruct \\\n    --api-key EMPTY \\\n    --dtype bfloat16 \\\n    --max-model-len 16000 \\ \n    --limit-mm-per-prompt image=1,video=0\n</code></pre> <p>Define a VLM engine to work with the vLLM server. </p> <pre><code>from vlm4ocr import BasicVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"Qwen/Qwen2.5-VL-7B-Instruct\", \n                             base_url=\"http://localhost:8000/v1\", \n                             api_key=\"EMPTY\",\n                             config=BasicVLMConfig(max_tokens=4096, temperature=0.0)    \n                            )\n</code></pre>"},{"location":"vlm_engines/#vlm-inference-with-api-servers","title":"VLM inference with API servers","text":"<p>Remote VLM inference servers are supported. We use OpenRouter as an example:</p> <pre><code>from vlm4ocr import BasicVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"Qwen/Qwen2.5-VL-7B-Instruct\", \n                             base_url=\"https://openrouter.ai/api/v1\", \n                             api_key=\"&lt;OPENROUTER_API_KEY&gt;\",\n                             config=BasicVLMConfig(max_tokens=4096, temperature=0.0)  \n                            )\n</code></pre>"},{"location":"vlm_engines/#ollama","title":"Ollama","text":"<p>Ollama to run VLM inference:</p> <pre><code>from vlm4ocr import BasicVLMConfig, OllamaVLMEngine\n\nvlm_engine = OllamaVLMEngine(model_name=\"llama3.2-vision:11b-instruct-fp16\", \n                             num_ctx:int=4096, \n                             keep_alive:int=300,\n                             config=BasicVLMConfig(max_tokens=4096, temperature=0.0)  \n                            )\n</code></pre>"},{"location":"vlm_engines/#openai-api","title":"OpenAI API","text":"<p>Follow the Best Practices for API Key Safety to set up API key.</p> <p></p><pre><code>export OPENAI_API_KEY=&lt;your_API_key&gt;\n</code></pre> For non-reasoning models (e.g., gpt-4o-mini), use <code>BasicVLMConfig</code> to set sampling parameters. <pre><code>from vlm4ocr import BasicVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"gpt-4o-mini\", config=BasicVLMConfig(max_tokens=4096, temperature=0.0) )\n</code></pre> <p>For reasoning models (e.g., o3-mini), use <code>OpenAIReasoningVLMConfig</code> to set reasoning effort.</p> <pre><code>from vlm4ocr import OpenAIReasoningVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"o3-mini\", config=OpenAIReasoningVLMConfig(reasoning_effort=\"low\") )\n</code></pre>"},{"location":"vlm_engines/#azure-openai-api","title":"Azure OpenAI API","text":"<p>Follow the Azure AI Services Quickstart to set up Endpoint and API key.</p> <pre><code>export AZURE_OPENAI_API_KEY=\"&lt;your_API_key&gt;\"\nexport AZURE_OPENAI_ENDPOINT=\"&lt;your_endpoint&gt;\"\n</code></pre> <p>For non-reasoning models (e.g., gpt-4o-mini), use <code>BasicVLMConfig</code> to set sampling parameters.</p> <pre><code>from llm_ie.engines import AzureOpenAIVLMEngine\n\nvlm_engine = AzureOpenAIVLMEngine(model=\"gpt-4o-mini\", \n                                  api_version=\"&lt;your api version&gt;\",\n                                  config=BasicVLMConfig(max_tokens=4096, temperature=0.0)  \n                                )\n</code></pre> <p>For reasoning models (e.g., o3-mini), use <code>OpenAIReasoningVLMConfig</code> to set reasoning effort.</p> <pre><code>from vlm4ocr import OpenAIReasoningVLMConfig, OpenAIVLMEngine\n\nvlm_engine = AzureOpenAIVLMEngine(model=\"o3-mini\", \n                                  api_version=\"&lt;your api version&gt;\",\n                                  config=OpenAIReasoningVLMConfig(reasoning_effort=\"low\") )\n</code></pre>"},{"location":"web_application/","title":"Web Application","text":"<p>A ready-to-use web application is included. It supports input preview, real-time streaming, and output export. </p> <p></p>"},{"location":"web_application/#installation","title":"Installation","text":""},{"location":"web_application/#running-with-docker","title":"Running with Docker","text":"<p>The easiest way to run VLM4OCR web application is through Docker. The image is available on Docker Hub. </p><pre><code>docker pull daviden1013/vlm4ocr-app:latest\ndocker run -p 5000:5000 daviden1013/vlm4ocr-app:latest\n</code></pre> Open your web browser and navigate to: http://localhost:5000 <p>If port 5000 is already in use on your machine, you can map it to a different local port. For example, to map it to local port 8080: </p><pre><code>docker run -p 8080:5000 daviden1013/vlm4ocr-app:latest\n</code></pre> Then visit http://localhost:8080"},{"location":"web_application/#using-ollama-with-the-dockerized-app","title":"Using Ollama with the Dockerized App","text":"<p>If you are running Ollama on your host machine (outside the Docker container) and want to connect to it from the VLM4OCR web app running inside Docker:</p> <p>Docker Desktop (Windows/Mac): In the VLM4OCR web UI, set the Ollama Host to http://host.docker.internal:11434. Linux: Run the Docker container with host networking: </p><pre><code>docker run --network=\"host\" daviden1013/vlm4ocr-app:latest\n</code></pre> With <code>--network=\"host\"</code>, you don't need the <code>-p</code> flag for port mapping (the container will use the host's network directly, so the app will be available at http://localhost:5000). Then, in the VLM4OCR web app, you can use the default http://localhost:11434 for the Ollama Host."},{"location":"web_application/#install-from-source","title":"Install from source","text":"<p>Alternatively, you can clone this repo and run VLM4OCR web application from source: </p><pre><code># Install python package\npip install vlm4ocr \n\n# Clone source code\ngit clone https://github.com/daviden1013/vlm4ocr.git\n\n# Run Web App\ncd vlm4ocr/services/web_app\npython run.py\n</code></pre>"},{"location":"web_application/#usage","title":"Usage","text":""},{"location":"web_application/#upload-image-pdf-tiff-for-ocr","title":"Upload image, PDF, TIFF for OCR","text":"<p>Drag-and-drop or click on the file upload area to select a file for upload. The file must be an image, PDF, or TIFF. Once uploaded, a preview will display in the input preview area on the right. </p> <p></p>"},{"location":"web_application/#select-output-mode","title":"Select output mode","text":"<p>Use the dropdown to select an output mode:</p> <ul> <li>plain text</li> <li>markdown</li> <li>HTML</li> </ul> <p></p>"},{"location":"web_application/#select-vlm-api","title":"Select VLM API","text":"<p>Use the dropdown to select an VLM API:</p> <ul> <li>OpenAI compatible</li> <li>OpenAI</li> <li>Azure OpenAI</li> <li>Ollama</li> </ul> <p>Once selected, further options such as API key, model name, and deployment will show.</p> <p></p>"},{"location":"web_application/#advanced-ocr-settings","title":"Advanced OCR settings","text":"<p>Write a user prompt to specify the document type and provide VLM with additional information. For example, \"The input is a comprehensive metabolic panel report\". Specify output tokens to manage computation resources and temperature to increase output stabability.</p> <p></p>"},{"location":"api/cli/","title":"CLI","text":""},{"location":"api/cli/#cli-reference","title":"CLI Reference","text":"<pre><code>usage: vlm4ocr [-h] --input_path INPUT_PATH\n               [--output_mode {markdown,HTML,text}]\n               [--output_path OUTPUT_PATH] [--skip_existing]\n               [--rotate_correction]\n               [--max_dimension_pixels MAX_DIMENSION_PIXELS] --vlm_engine\n               {openai,azure_openai,ollama,openai_compatible} --model MODEL\n               [--max_new_tokens MAX_NEW_TOKENS] [--temperature TEMPERATURE]\n               [--api_key API_KEY] [--base_url BASE_URL]\n               [--azure_api_key AZURE_API_KEY]\n               [--azure_endpoint AZURE_ENDPOINT]\n               [--azure_api_version AZURE_API_VERSION]\n               [--ollama_host OLLAMA_HOST] [--ollama_num_ctx OLLAMA_NUM_CTX]\n               [--ollama_keep_alive OLLAMA_KEEP_ALIVE]\n               [--user_prompt USER_PROMPT]\n               [--concurrent_batch_size CONCURRENT_BATCH_SIZE]\n               [--max_file_load MAX_FILE_LOAD] [--log] [--debug]\n\nVLM4OCR: Perform OCR on images, PDFs, or TIFF files using Vision Language\nModels. Processing is concurrent by default.\n\noptions:\n  -h, --help            show this help message and exit\n\nInput/Output Options:\n  --input_path INPUT_PATH\n                        Path to a single input file or a directory of files.\n                        (default: None)\n  --output_mode {markdown,HTML,text}\n                        Output format. (default: markdown)\n  --output_path OUTPUT_PATH\n                        Optional: Path to save OCR results. If input_path is a\n                        directory of multiple files, this should be an output\n                        directory. If input is a single file, this can be a\n                        full file path or a directory. If not provided,\n                        results are saved to the current working directory (or\n                        a sub-directory for logs if --log is used). (default:\n                        None)\n  --skip_existing       Skip processing files that already have OCR results in\n                        the output directory. (default: False)\n\nImage Processing Parameters:\n  --rotate_correction   Enable automatic rotation correction for input images.\n                        This requires Tesseract OCR to be installed and\n                        configured correctly. (default: False)\n  --max_dimension_pixels MAX_DIMENSION_PIXELS\n                        Maximum dimension (width or height) in pixels for\n                        input images. Images larger than this will be resized\n                        to fit within this limit while maintaining aspect\n                        ratio. (default: 4000)\n\nVLM Engine Options:\n  --vlm_engine {openai,azure_openai,ollama,openai_compatible}\n                        VLM engine. (default: None)\n  --model MODEL         Model identifier for the VLM engine. (default: None)\n  --max_new_tokens MAX_NEW_TOKENS\n                        Max new tokens for VLM. (default: 4096)\n  --temperature TEMPERATURE\n                        Sampling temperature. (default: 0.0)\n\nOpenAI &amp; OpenAI-Compatible Options:\n  --api_key API_KEY     API key. (default: None)\n  --base_url BASE_URL   Base URL for OpenAI-compatible services. (default:\n                        None)\n\nAzure OpenAI Options:\n  --azure_api_key AZURE_API_KEY\n                        Azure API key. (default: None)\n  --azure_endpoint AZURE_ENDPOINT\n                        Azure endpoint URL. (default: None)\n  --azure_api_version AZURE_API_VERSION\n                        Azure API version. (default: None)\n\nOllama Options:\n  --ollama_host OLLAMA_HOST\n                        Ollama host URL. (default: http://localhost:11434)\n  --ollama_num_ctx OLLAMA_NUM_CTX\n                        Context length for Ollama. (default: 4096)\n  --ollama_keep_alive OLLAMA_KEEP_ALIVE\n                        Ollama keep_alive seconds. (default: 300)\n\nOCR Engine Parameters:\n  --user_prompt USER_PROMPT\n                        Custom user prompt. (default: None)\n\nProcessing Options:\n  --concurrent_batch_size CONCURRENT_BATCH_SIZE\n                        Number of images/pages to process concurrently. Set to\n                        1 for sequential processing of VLM calls. (default: 4)\n  --max_file_load MAX_FILE_LOAD\n                        Number of input files to pre-load. Set to -1 for\n                        automatic config: 2 * concurrent_batch_size. (default:\n                        -1)\n  --log                 Enable writing logs to a timestamped file in the\n                        output directory. (default: False)\n  --debug               Enable debug level logging for console (and file if\n                        --log is active). (default: False)\n</code></pre>"},{"location":"api/ocr_engines/","title":"OCR Engine","text":""},{"location":"api/ocr_engines/#ocr-engine-reference","title":"OCR Engine Reference","text":""},{"location":"api/ocr_engines/#vlm4ocr.ocr_engines.OCREngine","title":"vlm4ocr.ocr_engines.OCREngine","text":"<pre><code>OCREngine(\n    vlm_engine: VLMEngine,\n    output_mode: str = \"markdown\",\n    system_prompt: str = None,\n    user_prompt: str = None,\n)\n</code></pre> <p>This class inputs a image or PDF file path and processes them using a VLM inference engine. Outputs plain text or markdown.</p> Parameters: <p>inference_engine : InferenceEngine     The inference engine to use for OCR. output_mode : str, Optional     The output format. Must be 'markdown', 'HTML', or 'text'. system_prompt : str, Optional     Custom system prompt. We recommend use a default system prompt by leaving this blank.  user_prompt : str, Optional     Custom user prompt. It is good to include some information regarding the document. If not specified, a default will be used.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/ocr_engines.py</code> <pre><code>def __init__(self, vlm_engine:VLMEngine, output_mode:str=\"markdown\", system_prompt:str=None, user_prompt:str=None):\n    \"\"\"\n    This class inputs a image or PDF file path and processes them using a VLM inference engine. Outputs plain text or markdown.\n\n    Parameters:\n    -----------\n    inference_engine : InferenceEngine\n        The inference engine to use for OCR.\n    output_mode : str, Optional\n        The output format. Must be 'markdown', 'HTML', or 'text'.\n    system_prompt : str, Optional\n        Custom system prompt. We recommend use a default system prompt by leaving this blank. \n    user_prompt : str, Optional\n        Custom user prompt. It is good to include some information regarding the document. If not specified, a default will be used.\n    \"\"\"\n    # Check inference engine\n    if not isinstance(vlm_engine, VLMEngine):\n        raise TypeError(\"vlm_engine must be an instance of VLMEngine\")\n    self.vlm_engine = vlm_engine\n\n    # Check output mode\n    if output_mode not in [\"markdown\", \"HTML\", \"text\"]:\n        raise ValueError(\"output_mode must be 'markdown', 'HTML', or 'text'\")\n    self.output_mode = output_mode\n\n    # System prompt\n    if isinstance(system_prompt, str) and system_prompt:\n        self.system_prompt = system_prompt\n    else:\n        prompt_template_path = importlib.resources.files('vlm4ocr.assets.default_prompt_templates').joinpath(f'ocr_{self.output_mode}_system_prompt.txt')\n        with prompt_template_path.open('r', encoding='utf-8') as f:\n            self.system_prompt =  f.read()\n\n    # User prompt\n    if isinstance(user_prompt, str) and user_prompt:\n        self.user_prompt = user_prompt\n    else:\n        prompt_template_path = importlib.resources.files('vlm4ocr.assets.default_prompt_templates').joinpath(f'ocr_{self.output_mode}_user_prompt.txt')\n        with prompt_template_path.open('r', encoding='utf-8') as f:\n            self.user_prompt =  f.read()\n\n    # Image processor\n    self.image_processor = ImageProcessor()\n</code></pre>"},{"location":"api/ocr_engines/#vlm4ocr.ocr_engines.OCREngine.stream_ocr","title":"stream_ocr","text":"<pre><code>stream_ocr(\n    file_path: str,\n    rotate_correction: bool = False,\n    max_dimension_pixels: int = None,\n) -&gt; Generator[Dict[str, str], None, None]\n</code></pre> <p>This method inputs a file path (image or PDF) and stream OCR results in real-time. This is useful for frontend applications. Yields dictionaries with 'type' ('ocr_chunk' or 'page_delimiter') and 'data'.</p> Parameters: <p>file_path : str     The path to the image or PDF file. Must be one of '.pdf', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp' rotate_correction : bool, Optional     If True, applies rotate correction to the images using pytesseract. max_dimension_pixels : int, Optional     The maximum dimension of the image in pixels. Original dimensions will be resized to fit in. If None, no resizing is applied.</p> Returns: <p>Generator[Dict[str, str], None, None]     A generator that yields the output:     {\"type\": \"info\", \"data\": msg}     {\"type\": \"ocr_chunk\", \"data\": chunk}     {\"type\": \"page_delimiter\", \"data\": page_delimiter}</p> Source code in <code>packages/vlm4ocr/vlm4ocr/ocr_engines.py</code> <pre><code>def stream_ocr(self, file_path: str, rotate_correction:bool=False, max_dimension_pixels:int=None) -&gt; Generator[Dict[str, str], None, None]:\n    \"\"\"\n    This method inputs a file path (image or PDF) and stream OCR results in real-time. This is useful for frontend applications.\n    Yields dictionaries with 'type' ('ocr_chunk' or 'page_delimiter') and 'data'.\n\n    Parameters:\n    -----------\n    file_path : str\n        The path to the image or PDF file. Must be one of '.pdf', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'\n    rotate_correction : bool, Optional\n        If True, applies rotate correction to the images using pytesseract.\n    max_dimension_pixels : int, Optional\n        The maximum dimension of the image in pixels. Original dimensions will be resized to fit in. If None, no resizing is applied.\n\n    Returns:\n    --------\n    Generator[Dict[str, str], None, None]\n        A generator that yields the output:\n        {\"type\": \"info\", \"data\": msg}\n        {\"type\": \"ocr_chunk\", \"data\": chunk}\n        {\"type\": \"page_delimiter\", \"data\": page_delimiter}\n    \"\"\"\n    # Check file path\n    if not isinstance(file_path, str):\n        raise TypeError(\"file_path must be a string\")\n\n    # Check file extension\n    file_ext = os.path.splitext(file_path)[1].lower()\n    if file_ext not in SUPPORTED_IMAGE_EXTS:\n        raise ValueError(f\"Unsupported file type: {file_ext}. Supported types are: {SUPPORTED_IMAGE_EXTS}\")\n\n    # Check if image preprocessing can be applied\n    if self.image_processor.has_tesseract==False and rotate_correction:\n        raise ImportError(\"pytesseract is not installed. Please install it to use rotate correction.\")\n\n    # PDF or TIFF\n    if file_ext in ['.pdf', '.tif', '.tiff']:\n        data_loader = PDFDataLoader(file_path) if file_ext == '.pdf' else TIFFDataLoader(file_path)\n        images = data_loader.get_all_pages()\n        # Check if images were extracted\n        if not images:\n            raise ValueError(f\"No images extracted from file: {file_path}\")\n\n        # OCR each image\n        for i, image in enumerate(images):\n            # Apply rotate correction if specified and tesseract is available\n            if rotate_correction and self.image_processor.has_tesseract:\n                try:\n                    image, _ = self.image_processor.rotate_correction(image)\n\n                except Exception as e:\n                    yield {\"type\": \"info\", \"data\": f\"Error during rotate correction: {str(e)}\"}\n\n            # Resize the image if max_dimension_pixels is specified\n            if max_dimension_pixels is not None:\n                try:\n                    image, _ = self.image_processor.resize(image, max_dimension_pixels=max_dimension_pixels)\n                except Exception as e:\n                    yield {\"type\": \"info\", \"data\": f\"Error resizing image: {str(e)}\"}\n\n            messages = self.vlm_engine.get_ocr_messages(self.system_prompt, self.user_prompt, image)\n            response_stream = self.vlm_engine.chat(\n                messages,\n                stream=True\n            )\n            for chunk in response_stream:\n                yield {\"type\": \"ocr_chunk\", \"data\": chunk}\n\n            if i &lt; len(images) - 1:\n                yield {\"type\": \"page_delimiter\", \"data\": get_default_page_delimiter(self.output_mode)}\n\n    # Image\n    else:\n        data_loader = ImageDataLoader(file_path)\n        image = data_loader.get_page(0)\n\n        # Apply rotate correction if specified and tesseract is available\n        if rotate_correction and self.image_processor.has_tesseract:\n            try:\n                image, _ = self.image_processor.rotate_correction(image)\n\n            except Exception as e:\n                yield {\"type\": \"info\", \"data\": f\"Error during rotate correction: {str(e)}\"}\n\n        # Resize the image if max_dimension_pixels is specified\n        if max_dimension_pixels is not None:\n            try:\n                image, _ = self.image_processor.resize(image, max_dimension_pixels=max_dimension_pixels)\n            except Exception as e:\n                yield {\"type\": \"info\", \"data\": f\"Error resizing image: {str(e)}\"}\n\n        messages = self.vlm_engine.get_ocr_messages(self.system_prompt, self.user_prompt, image)\n        response_stream = self.vlm_engine.chat(\n                messages,\n                stream=True\n            )\n        for chunk in response_stream:\n            yield {\"type\": \"ocr_chunk\", \"data\": chunk}\n</code></pre>"},{"location":"api/ocr_engines/#vlm4ocr.ocr_engines.OCREngine.sequential_ocr","title":"sequential_ocr","text":"<pre><code>sequential_ocr(\n    file_paths: Union[str, Iterable[str]],\n    rotate_correction: bool = False,\n    max_dimension_pixels: int = None,\n    verbose: bool = False,\n) -&gt; List[OCRResult]\n</code></pre> <p>This method inputs a file path or a list of file paths (image, PDF, TIFF) and performs OCR using the VLM inference engine.</p> Parameters: <p>file_paths : Union[str, Iterable[str]]     A file path or a list of file paths to process. Must be one of '.pdf', '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp' rotate_correction : bool, Optional     If True, applies rotate correction to the images using pytesseract. max_dimension_pixels : int, Optional     The maximum dimension of the image in pixels. Original dimensions will be resized to fit in. If None, no resizing is applied. verbose : bool, Optional     If True, the function will print the output in terminal.</p> Returns: <p>List[OCRResult]     A list of OCR result objects.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/ocr_engines.py</code> <pre><code>def sequential_ocr(self, file_paths: Union[str, Iterable[str]], rotate_correction:bool=False, \n                   max_dimension_pixels:int=None, verbose:bool=False) -&gt; List[OCRResult]:\n    \"\"\"\n    This method inputs a file path or a list of file paths (image, PDF, TIFF) and performs OCR using the VLM inference engine.\n\n    Parameters:\n    -----------\n    file_paths : Union[str, Iterable[str]]\n        A file path or a list of file paths to process. Must be one of '.pdf', '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'\n    rotate_correction : bool, Optional\n        If True, applies rotate correction to the images using pytesseract.\n    max_dimension_pixels : int, Optional\n        The maximum dimension of the image in pixels. Original dimensions will be resized to fit in. If None, no resizing is applied.\n    verbose : bool, Optional\n        If True, the function will print the output in terminal.\n\n    Returns:\n    --------\n    List[OCRResult]\n        A list of OCR result objects.\n    \"\"\"\n    if isinstance(file_paths, str):\n        file_paths = [file_paths]\n\n    ocr_results = []\n    for file_path in file_paths:\n        # Define OCRResult object\n        ocr_result = OCRResult(input_dir=file_path, output_mode=self.output_mode)\n        # get file extension\n        file_ext = os.path.splitext(file_path)[1].lower()\n        # Check file extension\n        if file_ext not in SUPPORTED_IMAGE_EXTS:\n            if verbose:\n                print(f\"{Fore.RED}Unsupported file type:{Style.RESET_ALL} {file_ext}. Supported types are: {SUPPORTED_IMAGE_EXTS}\")\n            ocr_result.status = \"error\"\n            ocr_result.add_page(text=f\"Unsupported file type: {file_ext}. Supported types are: {SUPPORTED_IMAGE_EXTS}\",\n                                image_processing_status={})\n            ocr_results.append(ocr_result)\n            continue\n\n        filename = os.path.basename(file_path)\n\n        try:\n            # Load images from file\n            if file_ext == '.pdf':\n                data_loader = PDFDataLoader(file_path) \n            elif file_ext in ['.tif', '.tiff']:\n                data_loader = TIFFDataLoader(file_path)\n            else:\n                data_loader = ImageDataLoader(file_path)\n\n            images = data_loader.get_all_pages()\n        except Exception as e:\n            if verbose:\n                print(f\"{Fore.RED}Error processing file {filename}:{Style.RESET_ALL} {str(e)}\")\n            ocr_result.status = \"error\"\n            ocr_result.add_page(text=f\"Error processing file {filename}: {str(e)}\", image_processing_status={})\n            ocr_results.append(ocr_result)\n            continue\n\n        # Check if images were extracted\n        if not images:\n            if verbose:\n                print(f\"{Fore.RED}No images extracted from file:{Style.RESET_ALL} {filename}. It might be empty or corrupted.\")\n            ocr_result.status = \"error\"\n            ocr_result.add_page(text=f\"No images extracted from file: {filename}. It might be empty or corrupted.\",\n                                image_processing_status={})\n            ocr_results.append(ocr_result)\n            continue\n\n        # OCR images\n        for i, image in enumerate(images):\n            image_processing_status = {}\n            # Apply rotate correction if specified and tesseract is available\n            if rotate_correction and self.image_processor.has_tesseract:\n                try:\n                    image, rotation_angle = self.image_processor.rotate_correction(image)\n                    image_processing_status[\"rotate_correction\"] = {\n                        \"status\": \"success\",\n                        \"rotation_angle\": rotation_angle\n                    }\n                    if verbose:\n                        print(f\"{Fore.GREEN}Rotate correction applied for {filename} page {i} with angle {rotation_angle} degrees.{Style.RESET_ALL}\")\n                except Exception as e:\n                    image_processing_status[\"rotate_correction\"] = {\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    }\n                    if verbose:\n                        print(f\"{Fore.RED}Error during rotate correction for {filename}:{Style.RESET_ALL} {rotation_angle['error']}. OCR continues without rotate correction.\")\n\n            # Resize the image if max_dimension_pixels is specified\n            if max_dimension_pixels is not None:\n                try:\n                    image, resized = self.image_processor.resize(image, max_dimension_pixels=max_dimension_pixels)\n                    image_processing_status[\"resize\"] = {\n                        \"status\": \"success\",\n                        \"resized\": resized\n                    }\n                    if verbose and resized:\n                        print(f\"{Fore.GREEN}Image resized for {filename} page {i} to fit within {max_dimension_pixels} pixels.{Style.RESET_ALL}\")\n                except Exception as e:\n                    image_processing_status[\"resize\"] = {\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    }\n                    if verbose:\n                        print(f\"{Fore.RED}Error resizing image for {filename}:{Style.RESET_ALL} {resized['error']}. OCR continues without resizing.\")\n\n            try:\n                messages = self.vlm_engine.get_ocr_messages(self.system_prompt, self.user_prompt, image)\n                response = self.vlm_engine.chat(\n                    messages,\n                    verbose=verbose,\n                    stream=False\n                )\n                # Clean the response if output mode is markdown\n                if self.output_mode == \"markdown\":\n                    response = clean_markdown(response)\n\n                # Add the page to the OCR result\n                ocr_result.add_page(text=response, \n                                    image_processing_status=image_processing_status)\n\n            except Exception as page_e:\n                ocr_result.status = \"error\"\n                ocr_result.add_page(text=f\"Error during OCR for a page in {filename}: {str(page_e)}\",\n                                    image_processing_status={})\n                if verbose:\n                    print(f\"{Fore.RED}Error during OCR for a page in {filename}:{Style.RESET_ALL} {page_e}\")\n\n        # Add the OCR result to the list\n        ocr_result.status = \"success\"\n        ocr_results.append(ocr_result)\n\n        if verbose:\n            print(f\"{Fore.BLUE}Successfully processed {filename} with {len(ocr_result)} pages.{Style.RESET_ALL}\")\n            for page in ocr_result:\n                print(page)\n                print(\"-\" * 80)\n\n    return ocr_results\n</code></pre>"},{"location":"api/ocr_engines/#vlm4ocr.ocr_engines.OCREngine.concurrent_ocr","title":"concurrent_ocr","text":"<pre><code>concurrent_ocr(\n    file_paths: Union[str, Iterable[str]],\n    rotate_correction: bool = False,\n    max_dimension_pixels: int = None,\n    concurrent_batch_size: int = 32,\n    max_file_load: int = None,\n) -&gt; AsyncGenerator[OCRResult, None]\n</code></pre> <p>First complete first out. Input and output order not guaranteed. This method inputs a file path or a list of file paths (image, PDF, TIFF) and performs OCR using the VLM inference engine.  Results are processed concurrently using asyncio.</p> Parameters: <p>file_paths : Union[str, Iterable[str]]     A file path or a list of file paths to process. Must be one of '.pdf', '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp' rotate_correction : bool, Optional     If True, applies rotate correction to the images using pytesseract. max_dimension_pixels : int, Optional     The maximum dimension of the image in pixels. Origianl dimensions will be resized to fit in. If None, no resizing is applied. concurrent_batch_size : int, Optional     The number of concurrent VLM calls to make.  max_file_load : int, Optional     The maximum number of files to load concurrently. If None, defaults to 2 times of concurrent_batch_size.</p> Returns: <p>AsyncGenerator[OCRResult, None]     A generator that yields OCR result objects as they complete.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/ocr_engines.py</code> <pre><code>def concurrent_ocr(self, file_paths: Union[str, Iterable[str]], rotate_correction:bool=False, \n                   max_dimension_pixels:int=None, concurrent_batch_size: int=32, max_file_load: int=None) -&gt; AsyncGenerator[OCRResult, None]:\n    \"\"\"\n    First complete first out. Input and output order not guaranteed.\n    This method inputs a file path or a list of file paths (image, PDF, TIFF) and performs OCR using the VLM inference engine. \n    Results are processed concurrently using asyncio.\n\n    Parameters:\n    -----------\n    file_paths : Union[str, Iterable[str]]\n        A file path or a list of file paths to process. Must be one of '.pdf', '.tif', '.tiff', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'\n    rotate_correction : bool, Optional\n        If True, applies rotate correction to the images using pytesseract.\n    max_dimension_pixels : int, Optional\n        The maximum dimension of the image in pixels. Origianl dimensions will be resized to fit in. If None, no resizing is applied.\n    concurrent_batch_size : int, Optional\n        The number of concurrent VLM calls to make. \n    max_file_load : int, Optional\n        The maximum number of files to load concurrently. If None, defaults to 2 times of concurrent_batch_size.\n\n    Returns:\n    --------\n    AsyncGenerator[OCRResult, None]\n        A generator that yields OCR result objects as they complete.\n    \"\"\"\n    if isinstance(file_paths, str):\n        file_paths = [file_paths]\n\n    if max_file_load is None:\n        max_file_load = concurrent_batch_size * 2\n\n    if not isinstance(max_file_load, int) or max_file_load &lt;= 0:\n        raise ValueError(\"max_file_load must be a positive integer\")\n\n    if self.image_processor.has_tesseract==False and rotate_correction:\n        raise ImportError(\"pytesseract is not installed. Please install it to use rotate correction.\")\n\n    return self._ocr_async(file_paths=file_paths, \n                           rotate_correction=rotate_correction,\n                           max_dimension_pixels=max_dimension_pixels,\n                           concurrent_batch_size=concurrent_batch_size, \n                           max_file_load=max_file_load)\n</code></pre>"},{"location":"api/vlm_engines/","title":"VLM Engines","text":""},{"location":"api/vlm_engines/#vlm-engines-reference","title":"VLM Engines Reference","text":""},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OllamaVLMEngine","title":"vlm4ocr.vlm_engines.OllamaVLMEngine","text":"<pre><code>OllamaVLMEngine(\n    model_name: str,\n    num_ctx: int = 8192,\n    keep_alive: int = 300,\n    config: VLMConfig = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>VLMEngine</code></p> <p>The Ollama inference engine.</p> Parameters: <p>model_name : str     the model name exactly as shown in &gt;&gt; ollama ls num_ctx : int, Optional     context length that LLM will evaluate. keep_alive : int, Optional     seconds to hold the LLM after the last API call. config : LLMConfig     the LLM configuration.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def __init__(self, model_name:str, num_ctx:int=8192, keep_alive:int=300, config:VLMConfig=None, **kwrs):\n    \"\"\"\n    The Ollama inference engine.\n\n    Parameters:\n    ----------\n    model_name : str\n        the model name exactly as shown in &gt;&gt; ollama ls\n    num_ctx : int, Optional\n        context length that LLM will evaluate.\n    keep_alive : int, Optional\n        seconds to hold the LLM after the last API call.\n    config : LLMConfig\n        the LLM configuration. \n    \"\"\"\n    if importlib.util.find_spec(\"ollama\") is None:\n        raise ImportError(\"ollama-python not found. Please install ollama-python (```pip install ollama```).\")\n\n    from ollama import Client, AsyncClient\n    self.client = Client(**kwrs)\n    self.async_client = AsyncClient(**kwrs)\n    self.model_name = model_name\n    self.num_ctx = num_ctx\n    self.keep_alive = keep_alive\n    self.config = config if config else BasicVLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OllamaVLMEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs VLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs VLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    options={'num_ctx': self.num_ctx, **self.formatted_params}\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat(\n                model=self.model_name, \n                messages=processed_messages, \n                options=options,\n                stream=True, \n                keep_alive=self.keep_alive\n            )\n            for chunk in response_stream:\n                content_chunk = chunk.get('message', {}).get('content')\n                if content_chunk:\n                    yield content_chunk\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.client.chat(\n                        model=self.model_name, \n                        messages=processed_messages, \n                        options=options,\n                        stream=True,\n                        keep_alive=self.keep_alive\n                    )\n\n        res = ''\n        for chunk in response:\n            content_chunk = chunk.get('message', {}).get('content')\n            print(content_chunk, end='', flush=True)\n            res += content_chunk\n        print('\\n')\n        return self.config.postprocess_response(res)\n\n    else:\n        response = self.client.chat(\n                            model=self.model_name, \n                            messages=processed_messages, \n                            options=options,\n                            stream=False,\n                            keep_alive=self.keep_alive\n                        )\n        res = response.get('message', {}).get('content')\n        return self.config.postprocess_response(res)\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OllamaVLMEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(messages: List[Dict[str, str]]) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]]) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.async_client.chat(\n                        model=self.model_name, \n                        messages=processed_messages, \n                        options={'num_ctx': self.num_ctx, **self.formatted_params},\n                        stream=False,\n                        keep_alive=self.keep_alive\n                    )\n\n    res = response['message']['content']\n    return self.config.postprocess_response(res)\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OllamaVLMEngine.get_ocr_messages","title":"get_ocr_messages","text":"<pre><code>get_ocr_messages(\n    system_prompt: str, user_prompt: str, image: Image\n) -&gt; List[Dict[str, str]]\n</code></pre> <p>This method inputs an image and returns the correesponding chat messages for the inference engine.</p> Parameters: <p>system_prompt : str     the system prompt. user_prompt : str     the user prompt. image : Image.Image     the image for OCR.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def get_ocr_messages(self, system_prompt:str, user_prompt:str, image:Image.Image) -&gt; List[Dict[str,str]]:\n    \"\"\"\n    This method inputs an image and returns the correesponding chat messages for the inference engine.\n\n    Parameters:\n    ----------\n    system_prompt : str\n        the system prompt.\n    user_prompt : str\n        the user prompt.\n    image : Image.Image\n        the image for OCR.\n    \"\"\"\n    base64_str = image_to_base64(image)\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\n            \"role\": \"user\",\n            \"content\": user_prompt,\n            \"images\": [base64_str]\n        }\n    ]\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OpenAIVLMEngine","title":"vlm4ocr.vlm_engines.OpenAIVLMEngine","text":"<pre><code>OpenAIVLMEngine(\n    model: str, config: VLMConfig = None, **kwrs\n)\n</code></pre> <p>               Bases: <code>VLMEngine</code></p> <p>The OpenAI API inference engine. Supports OpenAI models and OpenAI compatible servers: - vLLM OpenAI compatible server (https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)</p> <p>For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction</p> Parameters: <p>model_name : str     model name as described in https://platform.openai.com/docs/models config : VLMConfig, Optional     the VLM configuration. Must be a child class of VLMConfig.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def __init__(self, model:str, config:VLMConfig=None, **kwrs):\n    \"\"\"\n    The OpenAI API inference engine. Supports OpenAI models and OpenAI compatible servers:\n    - vLLM OpenAI compatible server (https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n\n    For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction\n\n    Parameters:\n    ----------\n    model_name : str\n        model name as described in https://platform.openai.com/docs/models\n    config : VLMConfig, Optional\n        the VLM configuration. Must be a child class of VLMConfig.\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import OpenAI, AsyncOpenAI\n    self.client = OpenAI(**kwrs)\n    self.async_client = AsyncOpenAI(**kwrs)\n    self.model = model\n    self.config = config if config else BasicVLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OpenAIVLMEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat.completions.create(\n                                    model=self.model,\n                                    messages=processed_messages,\n                                    stream=True,\n                                    **self.formatted_params\n                                )\n            for chunk in response_stream:\n                if len(chunk.choices) &gt; 0:\n                    if chunk.choices[0].delta.content is not None:\n                        yield chunk.choices[0].delta.content\n                    if chunk.choices[0].finish_reason == \"length\":\n                        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=processed_messages,\n            stream=True,\n            **self.formatted_params\n        )\n        res = ''\n        for chunk in response:\n            if len(chunk.choices) &gt; 0:\n                if chunk.choices[0].delta.content is not None:\n                    res += chunk.choices[0].delta.content\n                    print(chunk.choices[0].delta.content, end=\"\", flush=True)\n                if chunk.choices[0].finish_reason == \"length\":\n                    warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n        print('\\n')\n        return self.config.postprocess_response(res)\n    else:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=processed_messages,\n            stream=False,\n            **self.formatted_params\n        )\n        res = response.choices[0].message.content\n        return self.config.postprocess_response(res)\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OpenAIVLMEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(messages: List[Dict[str, str]]) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]]) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.async_client.chat.completions.create(\n        model=self.model,\n        messages=processed_messages,\n        stream=False,\n        **self.formatted_params\n    )\n\n    if response.choices[0].finish_reason == \"length\":\n        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n    res = response.choices[0].message.content\n    return self.config.postprocess_response(res)\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.OpenAIVLMEngine.get_ocr_messages","title":"get_ocr_messages","text":"<pre><code>get_ocr_messages(\n    system_prompt: str,\n    user_prompt: str,\n    image: Image,\n    format: str = \"png\",\n    detail: str = \"high\",\n) -&gt; List[Dict[str, str]]\n</code></pre> <p>This method inputs an image and returns the correesponding chat messages for the inference engine.</p> Parameters: <p>system_prompt : str     the system prompt. user_prompt : str     the user prompt. image : Image.Image     the image for OCR. format : str, Optional     the image format.  detail : str, Optional     the detail level of the image. Default is \"high\".</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def get_ocr_messages(self, system_prompt:str, user_prompt:str, image:Image.Image, format:str='png', detail:str=\"high\") -&gt; List[Dict[str,str]]:\n    \"\"\"\n    This method inputs an image and returns the correesponding chat messages for the inference engine.\n\n    Parameters:\n    ----------\n    system_prompt : str\n        the system prompt.\n    user_prompt : str\n        the user prompt.\n    image : Image.Image\n        the image for OCR.\n    format : str, Optional\n        the image format. \n    detail : str, Optional\n        the detail level of the image. Default is \"high\". \n    \"\"\"\n    base64_str = image_to_base64(image)\n    return [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/{format};base64,{base64_str}\",\n                        \"detail\": detail\n                    },\n                },\n                {\"type\": \"text\", \"text\": user_prompt},\n            ],\n        },\n    ]\n</code></pre>"},{"location":"api/vlm_engines/#vlm4ocr.vlm_engines.AzureOpenAIVLMEngine","title":"vlm4ocr.vlm_engines.AzureOpenAIVLMEngine","text":"<pre><code>AzureOpenAIVLMEngine(\n    model: str,\n    api_version: str,\n    config: VLMConfig = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>OpenAIVLMEngine</code></p> <p>The Azure OpenAI API inference engine. For parameters and documentation, refer to  - https://azure.microsoft.com/en-us/products/ai-services/openai-service - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart</p> Parameters: <p>model : str     model name as described in https://platform.openai.com/docs/models api_version : str     the Azure OpenAI API version config : LLMConfig     the LLM configuration.</p> Source code in <code>packages/vlm4ocr/vlm4ocr/vlm_engines.py</code> <pre><code>def __init__(self, model:str, api_version:str, config:VLMConfig=None, **kwrs):\n    \"\"\"\n    The Azure OpenAI API inference engine.\n    For parameters and documentation, refer to \n    - https://azure.microsoft.com/en-us/products/ai-services/openai-service\n    - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart\n\n    Parameters:\n    ----------\n    model : str\n        model name as described in https://platform.openai.com/docs/models\n    api_version : str\n        the Azure OpenAI API version\n    config : LLMConfig\n        the LLM configuration.\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import AzureOpenAI, AsyncAzureOpenAI\n    self.model = model\n    self.api_version = api_version\n    self.client = AzureOpenAI(api_version=self.api_version, \n                              **kwrs)\n    self.async_client = AsyncAzureOpenAI(api_version=self.api_version, \n                                         **kwrs)\n    self.config = config if config else BasicVLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"release_notes/v0.1.0/","title":":confetti_ball:First Release on GitHub:confetti_ball:","text":""},{"location":"release_notes/v0.1.0/#confetti_ballfirst-release-on-githubconfetti_ball","title":":confetti_ball:First Release on GitHub:confetti_ball:","text":""},{"location":"release_notes/v0.1.0/#sparklesoverview","title":":sparkles:Overview","text":"<p><code>vlm4ocr</code> provides a simple way to perform OCR using the power of modern Vision Language Models (VLMs). A drag-and-drop web application is included for easy access. The Python package supports concurrent batch processing for large amount of documents. CLI provides lightweight access to most OCR features without the burden of coding. </p>"},{"location":"release_notes/v0.1.0/#zapfeature-highlights","title":":zap:Feature Highlights","text":"<ul> <li>PDF </li> <li>TIFF</li> <li>image (PNG, JPG/JPEG, BMP, GIF, WEBP)</li> </ul> <p>Output formats:   - plain text   - Markdown   - HTML - ### Multiple flatforms: :snake:Python package, :earth_americas:Web App, and :computer:CLI   #### Web App   A Flask web App can be hosted locally for drag-and-drop access to OCR. </p> <p>https://github.com/user-attachments/assets/b196453c-fd2c-491a-ba1e-0a77cf7f5941   #### Python package   We provide Python classes <code>VLMEngine</code>'s to configure your preferred VLM inference engines hosted locally or remotely, and an <code>OCREngine</code> class with all functionality.    </p><pre><code>from vlm4ocr import OpenAIVLMEngine, OllamaVLMEngine, AzureOpenAIVLMEngine, OCREngine\n\nvlm_engine = OpenAIVLMEngine(model=\"Qwen/Qwen2.5-VL-7B-Instruct\", base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\nocr = OCREngine(vlm_engine, output_mode=\"markdown\")\n\n# Batch OCR for multiple image/pdf files\nocr_results = ocr.run_ocr([image_path, pdf_path], concurrent=True, concurrent_batch_size=32)\n</code></pre>   #### CLI    Command line interface (CLI) tool is installed automatically with the Python package, providing a lightweight access to most OCR features.    <pre><code># OpenAI compatible API (batch processing)\nvlm4ocr --input_path /examples/synthesized_data/ \\\n        --output_mode markdown \\\n        --vlm_engine openai_compatible \\\n        --model Qwen/Qwen2.5-VL-7B-Instruct \\\n        --api_key EMPTY \\\n        --base_url http://localhost:8000/v1 \\\n        --concurrent \\\n        --concurrent_batch_size 4 \\\n</code></pre>"},{"location":"release_notes/v0.1.0/#extensive-support-for-popular-models-and-platforms","title":"Extensive support for popular models and platforms","text":"We provide built-in support for OpenAI-compatible servers (e.g., vLLM, llama.cpp), Ollama, OpenAI API, and Azure. All open-weights VLMs and OpenAI models are supported. Latest VLMs including Qwen2.5-VL, Llama-3.2, LLaVa-1.5 are available."},{"location":"release_notes/v0.1.0/#high-throughput-batch-processing","title":"High throughput batch processing","text":"Concurrently process multiple images/pages with asynchronous inferencing. Balance performance and cost by setting <code>concurrent_batch_size</code>."},{"location":"release_notes/v0.1.0/#extensive-support-for-input-and-output-formats","title":"Extensive support for input and output formats","text":"Multiple image and file formats are supported:"},{"location":"release_notes/v0.1.0/#vertical_traffic_lightprerequisites","title":":vertical_traffic_light:Prerequisites","text":"<ul> <li>Python 3.x</li> <li>For PDF processing: poppler library.</li> <li>At least one VLM inference engine setup (Ollama, OpenAI/Azure API keys, or an OpenAI-compatible API endpoint).</li> </ul> <pre><code>pip install ollama # For Ollama\npip install openai # For OpenAI (compatible) and Azure OpenAI\n</code></pre>"},{"location":"release_notes/v0.1.0/#cd-installation","title":":cd: Installation","text":"<p>The Python package and CLI can be installed via  </p><pre><code>pip install vlm4ocr\n</code></pre> The web App is available on :whale:Docker Hub <pre><code>docker pull daviden1013/vlm4ocr-app:latest\ndocker run -p 5000:5000 daviden1013/vlm4ocr-app:latest\n</code></pre> Open your web browser and navigate to: http://localhost:5000"},{"location":"release_notes/v0.1.0/#page_facing_updocumentation","title":":page_facing_up:Documentation","text":"<p>For more details, please refer to our README.</p>"},{"location":"release_notes/v0.1.0/#your-feedback-is-highly-appreciated","title":"Your feedback is highly appreciated!!","text":""},{"location":"release_notes/v0.2.0/","title":"V0.2.0","text":""},{"location":"release_notes/v0.2.0/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v0.2.0/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v0.2.0/#added-image-processing-features","title":"Added image processing features","text":"<p>Added <code>rotate_correction</code> that use Tesseract to correct for rotation. <code>max_dimension_pixels</code> resize images to ensure the largest dimension (width or length) are less than the maximum allowed pixels. This manages the input image tokens.</p>"},{"location":"release_notes/v0.2.0/#image-processing-with-python","title":"Image processing with Python","text":"<pre><code>response = ocr.concurrent_ocr(file_paths=&lt;a list of files&gt;, \n                              rotate_correction=True,\n                              max_dimension_pixels=4000,\n                              concurrent_batch_size=4,\n                              max_file_load=8)\n</code></pre>"},{"location":"release_notes/v0.2.0/#image-processing-in-cli","title":"Image processing in CLI","text":"<pre><code>vlm4ocr --input_path /examples/synthesized_data/ \\\n        --output_mode markdown \\\n        --max_dimension_pixels 4000 \\\n        --rotate_correction \\\n        --vlm_engine openai_compatible \\\n        --model Qwen/Qwen2.5-VL-7B-Instruct \\\n        --api_key EMPTY \\\n        --base_url http://localhost:8000/v1 \\\n        --concurrent_batch_size 4\n</code></pre>"},{"location":"release_notes/v0.2.0/#implemented-asynchronous-ocr-output","title":"Implemented asynchronous OCR output","text":"<p>We added <code>concurrent_ocr</code> method to <code>OCREngine</code> that returns an async generator of <code>OCRResult</code> instance (<code>AsyncGenerator[OCRResult, None]</code>). OCR results are generated whenever is ready (first-done-first-out). There is no guarantee the input order and output order will match. Use the <code>OCRResult.filename</code> as identifier. </p>"},{"location":"release_notes/v0.2.0/#example-dynamic-output-writing","title":"Example: dynamic output-writing","text":"<p>The example below use <code>concurrent_ocr</code> to perform OCR and write available results to file.</p> <pre><code>import asyncio\n\nasync def run_ocr():\n    response = ocr.concurrent_ocr(&lt;list of files&gt;, concurrent_batch_size=4)\n    async for result in response:\n        if result.status == \"success\":\n            filename = result.filename\n            ocr_text = result.to_string()\n            with open(f\"{filename}.md\", \"w\", encoding=\"utf-8\") as f:\n                f.write(ocr_text)\n\nasyncio.run(run_ocr())\n</code></pre>"},{"location":"release_notes/v0.2.0/#optimized-file-staging","title":"Optimized file staging","text":"<p>We added <code>max_file_load</code> parameter to <code>concurrent_ocr</code> method. The <code>max_file_load</code> manages files pre-loading (staged and waiting for OCR). The <code>concurrent_batch_size</code> manages the number of images/pages VLM processes at a time. The <code>max_file_load</code> and <code>concurrent_batch_size</code> should be tuned to optimize system performance and avoid bottleneck. </p>"},{"location":"release_notes/v0.2.0/#changes","title":"Changes","text":""},{"location":"release_notes/v0.2.0/#moved-vlm-sampling-parameters-to-vlmconfig","title":"Moved VLM sampling parameters to <code>VLMConfig</code>","text":"<p>To support different VLMs, especially reasoning models, we moved sampling parameters (e.g., temperature, max_new_tokens) from <code>OCREngine</code> to <code>VLMConfig</code>. Now, those parameters can be set by passing a configuration instance to the <code>VLMEngine</code>. </p> <pre><code>from vlm4ocr import BasicVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"Qwen/Qwen2.5-VL-7B-Instruct\", \n                             base_url=\"http://localhost:8000/v1\", \n                             api_key=\"EMPTY\",\n                             config=BasicVLMConfig(max_tokens=4096, temperature=0.0)    \n                            )\n</code></pre> <p>For reasoning models (e.g., o3-mini), use <code>OpenAIReasoningVLMConfig</code> to set reasoning effort.</p> <pre><code>from vlm4ocr import OpenAIReasoningVLMConfig, OpenAIVLMEngine\n\nvlm_engine = OpenAIVLMEngine(model=\"o3-mini\", config=OpenAIReasoningVLMConfig(reasoning_effort=\"low\") )\n</code></pre>"}]}